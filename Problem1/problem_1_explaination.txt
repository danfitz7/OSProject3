Each type of job (TS, S or U) gets it's own queue to get in to a cluster half.
The scheduler keeps track of what jobs are currently running in each cluster half, and blocks on a mutex that either job can unlock.
When a job finishes in one of the cluster halves, it unlocks the mutex to signal to the scheduler to put another job in.
The scheduler will choose to put in a TS job if there are 3 or more TS jobs in the TS queue, or if there are only 2 but it just put one in last time.
(This way if there are 3 or more TS jobs, the next to jobs to run will be TS)

Otherwise, if there is a T or TS job still running in the other cluster half, the scheduler will plan to run the next queue S job.
If the scheduler doesn't have to run a TS job and the remaining job is U, it will plan to run the next queue U job.

These normal plans can be superseded by the below "fairness" logic.
To avoids depriving any job type of the cluster, when any job finishes in one of the halves, we check how many jobs of each type are in their respective queues.
If the remaining job in the other half is secret (TS or S), but number of unclassified (U) jobs queued is more than twice the number of secret (TS or S) jobs queues, then a flag is set to let the remaining job finish, then start letting unclassified jobs go.
The reverse happens if there is a U job remaining in the cluster, but the number of TS or S jobs is more than twice the number of U jobs queued.

When the job type must be switched (Us were running but we must run S/TS or vice-versa), the remaining job of the original type is allowed to finish 
(so that both clusters are empty) then the scheduler switches to putting the new type of jobs in the cluster halves.
